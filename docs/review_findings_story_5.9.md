# Adversarial Review Findings: Story 5.9 (AI Cover Generator) - Re-Review

1.  **Missing Mobile Design Specs (Design/UX)**: The spec details a "Cover Lab" and complex UI interactions (overlays, side-by-side comparisons) but completely ignores mobile responsiveness. A 4-up image grid with side-by-side comparisons will break on mobile screens without specific "stacked" or "carousel" behavior definitions.
2.  **Ambiguous Image Format Requirements (Technical)**: The spec mentions "PNG" in one task path example but doesn't explicitly mandate the output format for R2. Imagen might default to WebP or JPEG. If high-fidelity print usage is ever intended, PNG is required; if web-only, WebP is preferred for performance. The discrepancy needs resolution.
3.  **"Save to Gallery" Permission Scope (Security)**: The spec adds "Save to Gallery" but doesn't define the ACLs for the `gallery/` path. Are these images public? Private? If private, how is access controlled via R2 presigned URLs vs. public buckets? Publicly accessible user-generated content (even text-less) carries moderation risks.
4.  **Font Licensing & File Bloat (Legal/Perf)**: The "UI Text Overlay" feature implies loading custom fonts. Where do these fonts come from? Are they Google Fonts (GDPR/privacy implications) or self-hosted? Loading multiple high-quality display fonts for a cover editor can significantly degrade page load performance if not lazy-loaded.
5.  **Lack of "Edit" Capability for Overlays (UX)**: The spec mentions "customizable font/position controls" but doesn't specify if these settings are *saved*. If a user generates a cover, sets the text, leaves, and comes back, do they lose their text layout? The metadata saves the *image* prompt, but not the *overlay* configuration.
6.  **Polling vs. Server-Sent Events/Websockets (Architecture)**: The spec mandates "polling" for the async job. While simple, rapid polling by multiple users can thrash the DB/API. SSE (Server-Sent Events) is a more efficient pattern for status updates in Next.js/Vercel environments and should be considered or explicitly ruled out with a polling interval cap (e.g., "poll every 3s, backoff to 10s").
7.  **Undefined "Job" Cleanup (Ops)**: We have R2 lifecycle rules for *images*, but what about the Supabase *job records*? If we create a job record for every generation, the `cover_jobs` table will grow indefinitely. A retention policy for the database records is missing.
8.  **Quota "Documentation" vs. Enforcement (Ops)**: The spec says "Document the project's Imagen QPM limit." This is passive. If the limit is hit, the code will throw errors. The spec needs to mandate *handling* the 429 Too Many Requests error specifically, possibly with a "High demand, please wait..." retry-after logic, rather than just "documenting" it.
9.  **No "Regenerate" Workflow (UX)**: The standard AI workflow is "Generate -> Iterate". The current spec implies a linear "Generate -> Select" flow. Is there a "Use this prompt as a starting point" or "Generate more like this" button? The lack of iteration frictionlessly makes the tool feel rigid.
10. **Accessibility of Generated Content (A11y)**: The generated images are visual-only. The spec needs to mandate that the *user description* is automatically used as the `alt` text for the generated image in the UI, otherwise, the application fails WCAG compliance for screen reader users.
11. **Concurrency Edge Case (Logic)**: What happens if a user clicks "Generate", then immediately clicks "Generate" again before the first batch returns? The rate limit is 5/day, but can they run 2 parallel jobs? The UI state needs to lock the "Generate" button or handle multi-job polling.
12. **Inconsistent "Skeleton" vs "Progressive" Loading (UX)**: Task 2.1 says "returns job ID", UI "polls". Task 1.5 says "upload each... as it completes". The UI needs to handle a mix of "Loading..." and "Image 1 Ready". The spec is slightly vague on whether the poll response includes *partial* arrays or only the final state. It should explicitly support partial array updates.